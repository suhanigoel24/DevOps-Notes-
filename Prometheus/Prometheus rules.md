üëâ¬†*‚ÄúWhere do these labels (tags) come from in the first place, and how does relabeling affect which ones I can use in alert rules?‚Äù*

Let‚Äôs break it down clearly:

* * *

# üîπ 1. Where do labels come from during scraping?

Every target Prometheus scrapes gets labels from **3 sources**:

1.  **Static labels from the scrape config**
    
    ```yaml
    scrape_configs:
    - job_name: node_exporter
      static_configs:
      - targets: ["10.4.51.236:9100"]
        labels:
          environment: prod
          techteam: digitalcst
    ```
    
    ‚Üí Adds `environment="prod"`, `techteam="digitalcst"` to every metric scraped from that target.
    

* * *

2.  **Service discovery (SD) labels**  
    If you use **Kubernetes SD** or **EC2 SD**, Prometheus automatically attaches **meta labels** like:
    
    - Kubernetes: `__meta_kubernetes_namespace`, `__meta_kubernetes_node_name`, `__meta_kubernetes_pod_label_app`, etc.
        
    - EC2: `__meta_ec2_instance_id`, `__meta_ec2_tag_Name`, `__meta_ec2_private_ip`, etc.
        
    
    Example (K8s):
    
    ```
    __meta_kubernetes_namespace="payments"
    __meta_kubernetes_pod_name="nginx-12345"
    __meta_kubernetes_pod_label_app="frontend"
    ```
    

* * *

3.  **Default internal labels**  
    Always present:
    
    - `job` ‚Üí from `job_name` in scrape config
        
    - `instance` ‚Üí from the target‚Äôs address (`host:port`)
        
    - `__address__` ‚Üí raw target address before relabeling
        

* * *

# üîπ 2. How does relabeling fit in?

üëâ **Relabeling transforms labels before final scrape happens**.  
It can:

- Copy from `__meta_*` into a real label (e.g. `namespace`, `app`)
    
- Overwrite or drop labels
    
- Drop the entire target
    

Example (K8s relabel):

```yaml
relabel_configs:
- source_labels: [__meta_kubernetes_namespace]
  target_label: namespace
- source_labels: [__meta_kubernetes_pod_label_app]
  target_label: app
```

Result:  
Every scraped metric now has:

```
namespace="payments"
app="frontend"
```

These are **real labels** you can use in alerts.

* * *

# üîπ 3. Which labels do you use in rules?

You only use **final labels after relabeling**, because those are what get attached to your scraped metrics.

‚úÖ Example:

```yaml
expr: up{namespace="payments", app="frontend"} == 0
```

If you didn‚Äôt add the relabel above, then `namespace` and `app` wouldn‚Äôt exist ‚Üí your rule would never match.

* * *

# üîπ 4. Practical difference

- **Before relabeling (raw SD labels):**
    
    ```
    __meta_kubernetes_namespace="payments"
    __meta_kubernetes_pod_label_app="frontend"
    ```
    
- **After relabeling:**
    
    ```
    namespace="payments"
    app="frontend"
    ```
    

‚ö†Ô∏è Rules and alerts should **never use `__meta_*` labels**, because those only exist internally during target discovery.  
You must relabel them into real labels if you want to use them.

* * *

# ‚ö° TL;DR

- Labels/tags come from **static configs**, **service discovery meta labels**, and **default Prometheus labels**.
    
- Relabeling transforms/discards these before scraping.
    
- You should always use the **post-relabel labels** (like `environment`, `namespace`, `app`) in your alert rules.
    

* * *

&nbsp;

üëç This is about¬†**label matchers** in **PromQL** (and also in alerting rules).

* * *

# üîπ PromQL Label Match Operators

Prometheus lets you filter time series by labels. There are 4 operators:

| Operator | Meaning | Example | Matches |
| --- | --- | --- | --- |
| `=` | Exact match | `environment="prod"` | Only metrics where `environment` label is exactly `prod`. |
| `!=` | Not equal | `job!="kafka"` | Metrics where `job` is anything except `kafka`. |
| `=~` | Regex match | \`environment=~"prod | Prod |
| `!~` | Regex not match | \`job!~"kafka | kafka_exporter"\` |

* * *

# üîπ Your Case

In your alert rule:

```promql
up{job!~"kafka|kafka_exporter", environment=~"prod|Prod|production", techteam="digitalcst"} == 0
```

- `job!~"kafka|kafka_exporter"`  
    ‚Üí Drop any target whose `job` is `kafka` or `kafka_exporter`.
    
- `environment=~"prod|Prod|production"`  
    ‚Üí Only include targets where `environment` is one of `prod`, `Prod`, or `production`.  
    (`=~` uses regex `|` for OR).
    
- `techteam="digitalcst"`  
    ‚Üí Exact match; only targets with `techteam` label equal to `"digitalcst"`.
    

* * *

# ‚ö° TL;DR

- `=` ‚Üí exact match
    
- `!=` ‚Üí exact non-match
    
- `=~` ‚Üí regex match (like ‚Äúone of these patterns‚Äù)
    
- `!~` ‚Üí regex non-match (exclude these patterns)
    

* * *

&nbsp;

* * *

# üîπ Your Alert Rule

```yaml
- alert: AWS.SERVICE.EndpointDown
  expr: up{job!~"kafka|kafka_exporter", environment=~"prod|Prod|production", techteam="digitalcst"} == 0
  for: 2m
  labels:
    priority: P2
    severity: warning
    techteam: digitalcst
    app: '{{ $labels.app}}'
  annotations:
    summary: "Please check if telegraf/exporter is running on `{{ $labels.instance }}` is down for more than 2 mins"
```

* * *

### üîë What it means:

1.  **`alert: AWS.SERVICE.EndpointDown`**
    
    - Name of the alert rule.
2.  **`expr`**:
    
    ```promql
    up{job!~"kafka|kafka_exporter", environment=~"prod|Prod|production", techteam="digitalcst"} == 0
    ```
    
    - `up` = standard Prometheus metric set to `1` if the scrape succeeded, `0` if it failed.
        
    - The filter `{...}` keeps only time series where:
        
        - `job` **is not** matching `kafka` or `kafka_exporter`.
            
        - `environment` is `prod`, `Prod`, or `production`.
            
        - `techteam` = `digitalcst`.
            
    - Then check `== 0` ‚Üí meaning "the target is **down**".
        
3.  **`for: 2m`**
    
    - Target must stay down for **2 minutes continuously** before firing the alert.
        
    - Prevents false alarms if a scrape fails briefly.
        
4.  **Labels:**
    
    - Extra metadata attached to the alert:
        
        - `priority=P2`
            
        - `severity=warning`
            
        - `techteam=digitalcst`
            
        - `app={{ $labels.app }}` ‚Üí dynamically filled with the `app` label of the target.
            
5.  **Annotations:**
    
    - Human-readable message for dashboards/Alertmanager.
        
    - `{{ $labels.instance }}` will be replaced with the instance IP:port of the failed target.
        

‚úÖ So the alert basically says:  
üëâ "If any **non-kafka prod target owned by digitalcst** is down for more than 2 minutes, fire an alert."

* * *

# üîπ How to check if this alert rule applies to a particular instance

Say you have instance `10.4.51.236:9273`.  
You want to know if it will be caught by this rule.

### Step 1: Check labels of that instance

Go to Prometheus **Targets page** (`http://<prometheus>:9090/targets`) or query in PromQL:

```promql
up{instance="10.4.51.236:9273"}
```

This shows you:

- Is it being scraped? (up=1 or 0)
    
- What labels it has (`job`, `environment`, `techteam`, `app`, ‚Ä¶).
    

* * *

### Step 2: Test the alert expression for that instance

Run in Prometheus UI:

```promql
up{instance="10.4.51.236:9273", job!~"kafka|kafka_exporter", environment=~"prod|Prod|production", techteam="digitalcst"}
```

- If you get a series ‚Üí that instance matches the filters.
    
- If you get **nothing**, then the rule does not apply to that instance (maybe wrong `environment` tag, missing `techteam`, or `job` excluded).
    

* * *

### Step 3: Simulate the condition

To see if it *would* alert:

```promql
up{instance="10.4.51.236:9273", job!~"kafka|kafka_exporter", environment=~"prod|Prod|production", techteam="digitalcst"} == 0
```

- If result is `1` ‚Üí alert condition is met (target down).
    
- If result is empty ‚Üí rule does not apply.
    
- If result is `0` ‚Üí target is up, so rule won‚Äôt fire.
    

* * *

# ‚ö° TL;DR

- **The rule alerts when a prod digitalcst instance (non-kafka) is down for 2m.**
    
- **To check if it applies to an instance**:
    
    1.  Look at its labels (`up{instance="X"}`)
        
    2.  See if they match the rule filters.
        
    3.  Test the rule‚Äôs expression manually in Prometheus UI.
        

* * *

Prometheus periodically sends all *active alerts* to Alertmanager over HTTP.  
The payload looks like:

```json
[
  {
    "labels": {
      "alertname": "AWS.SERVICE.EndpointDown",
      "instance": "10.4.51.236:9273",
      "job": "node_exporter",
      "environment": "prod",
      "severity": "warning",
      "techteam": "digitalcst"
    },
    "annotations": {
      "summary": "Please check if telegraf/exporter is running on `10.4.51.236:9273`"
    },
    "startsAt": "2025-09-09T11:25:00Z",
    "endsAt": "0001-01-01T00:00:00Z",
    "generatorURL": "http://prometheus:9090/graph?g0.expr=...",
    "status": "firing"
  }
]
```

So Alertmanager **doesn‚Äôt calculate labels** ‚Äî it just receives whatever Prometheus sends:

- **Series labels** (from the metric, like `instance`)
    
- **Rule labels** (from your alert rule)
    
- **Annotations** (after template substitution)
    

* * *

&nbsp;

&nbsp;

Great question üëç You‚Äôre asking specifically about **`net_response_result_code`** (from the Telegraf `net_response` input plugin) and how it indicates if a port is down.

* * *

## üîπ What is `net_response` in Telegraf?

- The **`net_response` plugin** in Telegraf actively tests TCP/UDP/HTTP connections to remote hosts.
    
- It connects to a host + port you specify and reports the results as metrics.
    

One of the key metrics is:

```
net_response_result_code
```

* * *

## üîπ Meaning of `net_response_result_code`

- `0` ‚Üí **success** (connection worked, port is open)
    
- `1` ‚Üí **timeout** (port unreachable or filtered)
    
- `2` ‚Üí **connection refused** (port closed)
    
- `3` ‚Üí **DNS error** (if host cannot be resolved)
    

So, **any non-zero code means the port is down or not working correctly**.

* * *

## üîπ How to use it for alerts

You can create PromQL alerts like:

‚úÖ Check if port is **down**:

```promql
net_response_result_code{job="netcheck", instance="10.4.51.236", port="5060"} != 0
```

This means: if the result code is anything other than `0` ‚Üí alert.

‚úÖ Example alert rule:

```yaml
- alert: PortDown
  expr: net_response_result_code{port="5060"} != 0
  for: 2m
  labels:
    severity: critical
    team: network
  annotations:
    summary: "Port 5060 is DOWN on {{ $labels.instance }}"
    description: "Telegraf net_response failed to connect to {{ $labels.instance }}:{{ $labels.port }}"
```

‚úÖ Or, the inverse check (only succeed if `== 0`):

```promql
net_response_result_code{port="5060"} == 0
```

You‚Äôd flip the logic depending on whether you alert on failure or success.

* * *

## üîπ Why use it instead of `up` metric?

- `up` (from Prometheus scrape) only tells you if **Prometheus itself** could scrape an exporter.
    
- `net_response_result_code` lets you actively check **any arbitrary port or service**, even if it doesn‚Äôt expose Prometheus metrics.
    

Example: SIP (5060), MySQL (3306), custom app port, etc.

* * *

‚úÖ **So, TL;DR:**

- `net_response_result_code = 0` ‚Üí port is UP.
    
- `net_response_result_code != 0` ‚Üí port is DOWN (timeout, refused, etc.).
    
- You use that in alert rules to detect port availability.
    

* * *

üëâ Do you want me to show you how to **configure Telegraf‚Äôs `[[inputs.net_response]]`** for, say, SIP port `5060`, so you can generate this metric in the first place?

Good catch ‚ö° ‚Äî Telegraf works with **plugins**, and they are not all enabled by default.

* * *

## üîπ How Telegraf plugins work

- Telegraf ships with a **lot of input plugins** (CPU, memory, disk, net, docker, k8s, etc.).
    
- Only the ones you configure in `telegraf.conf` (or included `.d` configs) are enabled.
    
- So: **the binary has `net_response` built in**, but you must **add and configure it** in your config.
    

* * *

## üîπ `net_response` plugin config example

If you want to monitor a port (say SIP `5060` or HTTP `80`), you‚Äôd add this block in `/etc/telegraf/telegraf.conf` or `/etc/telegraf/telegraf.d/net_response.conf`:

```toml
[[inputs.net_response]]
  ## Address (host:port) to connect to
  address = "udp://10.4.51.236:5060"
  ## OR for TCP
  # address = "tcp://10.4.51.236:3306"

  ## Timeout for the connection test
  timeout = "5s"

  ## Optional: expected string in response (for HTTP, etc.)
  # read_timeout = "3s"
  # send = "GET /health HTTP/1.0\r\n\r\n"
  # expect = "200 OK"

  ## Set labels (tags) for Prometheus
  tags = { port = "5060", service = "sip", environment = "prod" }
```

* * *

## üîπ What happens after enabling?

- Telegraf will probe the port and export metrics like:

```
net_response_result_code{host="10.4.51.236", port="5060", protocol="udp"} 0
net_response_response_time{host="10.4.51.236", port="5060", protocol="udp"} 0.0012
```

- Prometheus scrapes Telegraf ‚Üí you can now alert on `net_response_result_code`.

* * *

## üîπ TL;DR

- `net_response` plugin is included with Telegraf, but **not active by default**.
    
- You must add `[[inputs.net_response]]` blocks for each host:port you want to check.
    
- Once enabled, Prometheus sees `net_response_result_code` and you can use it in alert rules.
    

* * *

üëâ Do you want me to draft a **ready-to-use Telegraf config** for checking multiple ports (like MySQL 3306, SIP 5060, Redis 6379) so you can just drop it in?